<!doctype html>
<html lang="en">
<head>
<title>Image Colorization</title>
<meta property="og:title" content=DDColor" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">DD Color</nobr>
 <nobr class="widenobr">For DS4440</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders</h2>

</div>
</div>
<div class="row">
<div class="col">

<section id="introduction"> <h1>1. Introduction</h1> <p>The goal of this project is to investigate the potential of turning the DDColor method, proposed by Kang et al. in their paper "DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders," [1] into a practical product for image colorization. Specifically, we aim to explore the following two main questions:</p> <p><strong>1. Can the DDColor method be effectively applied to new, diverse datasets beyond the benchmarks used in the original paper?</strong></p> <p>While the authors demonstrated the superior performance of DDColor on datasets like ImageNet, COCO-Stuff, and ADE20K, it is essential to evaluate the method's generalization capability across a broader range of image domains and scenarios. By testing DDColor on new datasets, we can assess its robustness and identify potential limitations or areas for improvement.</p> <p><strong>2. Is it possible to achieve comparable performance with a reduced training data requirement?</strong></p> <p>Training deep learning models often demands large-scale datasets, which can be challenging to obtain, especially in specialized domains. In this project, we will investigate whether the DDColor method can be fine-tuned or adapted to achieve satisfactory results with a smaller training dataset, potentially making it more accessible and resource-efficient for real-world applications.</p> <p>Through this project, we aim to gain insights into the practical applicability of the DDColor method and explore potential avenues for enhancing its performance, efficiency, and scalability. By addressing these questions, we hope to contribute to the development of effective and accessible image colorization solutions that can benefit various industries and applications.</p> </section>

<section id="review"> <h1>2. Review of DDColor Paper</h1> <p>The paper "DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders" by Kang et al. presents a novel end-to-end method for automatic image colorization called DDColor. The main novelty of DDColor lies in its dual decoder architecture, consisting of a pixel decoder and a color decoder.</p></p> <p> <img src="architecture.png" alt="DDColor Architecture" class="img-fluid"> <br> </p> <p></p> <p>DDColor represents a significant advancement in automatic image colorization by learning color representations in an end-to-end manner without relying on manually designed priors. The model exhibits good generalization ability and can handle diverse objects and contexts, making it a promising solution for real-world applications such as legacy photo restoration, video remastering, and art creation.</p> <p>The pixel decoder restores the spatial resolution of the image by gradually upsampling the features extracted from the encoder. Crucially, the color decoder learns semantic-aware color queries from multi-scale visual features, eliminating the need for hand-crafted priors used in previous methods. The color decoder utilizes a query-based transformer to establish correlations between color representations and multi-scale semantic representations via cross-attention, effectively alleviating color bleeding issues prevalent in existing approaches.</p> <p>Additionally, the authors introduce a simple yet effective colorfulness loss to enhance the color richness of the generated results. Through extensive experiments on public benchmarks like ImageNet, COCO-Stuff, and ADE20K, the authors demonstrate that DDColor achieves superior performance over state-of-the-art methods, both quantitatively (in terms of metrics like FID and colorfulness scores) and qualitatively (producing more natural, vivid, and semantically consistent colorization results, even for complex scenes).</p></section>

<section id="implementation">
 <h1>3. Implementation</h1>
 <p>Tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean eu libero sit amet quam egestas semper sit amet quam egestas semper. Aenean ultricies mi ultricies mi ultricies vitae est. Mauris placerat eleifend leo.</p>
 <h3>Encoder</h3>
 <p>Similarly to the original paper, we use ConvNeXT [2] as our backbone. However, unlike the paper, due to our limited resources, we use the smallest version of the ConvNeXT, ConvNeXT-Tiny, rather than the much larger model they use, ConvNeXT-XLarge. Furthermore, unlike the paper, we keep our encoder frozen in order to speed up training.</p>
 <p>The output from the backbone is four tensors of dimensions (96, H/4, W/4), (192, H/8, W/8), (384, H/16, W/16), and (768, H/32, W/32), which represent the hidden states of the model. All four tensors are used as direct inputs or shortcut inputs to the pixel decoder.</p>
 <h3>Pixel Decoder</h3>
 <p>Pellentesque et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo. Quisque sit amet est et sapien ullamcorper pharetra. Vestibulum erat wisi, condimentum sed, commodo vitae, ornare sit amet, wisi. Aenean fermentum, elit eget tincidunt condimentum, eros ipsum rutrum orci, sagittis tempus lacus enim ac dui. Donec non enim in turpis pulvinar facilisis. Ut felis. Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat. Aliquam erat volutpat. Nam dui mi, tincidunt quis, accumsan porttitor, facilisis luctus, metus</p>
 <p>Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo.</p>
 <h3>Color Decoder</h3>
 <p>As in the paper, the color decoder consists of several blocks, each taking in as input three tensors of shapes (512, H/16, W/16), (512, H/8, W/8), and (256, H/4, W/4), which come from the different upsampling stages of the pixel decoder. The output of this module is a (K, C) tensor, where K is a hyperparameter representing the number of color queries, and C is the embedding dimension.</p>
 <p>The main input to each color decoder block is a tensor called the color queries. It has shape (K, C), and the output of each block is essentially an updated representation of these color queries. The color query input to the very first block is a fully learned tensor that is zero-initialized. Each layer of the color decoder comprises three blocks, one for each of the three inputs coming from the pixel decoder. Each block has two inputs, the color queries and the image features. Aside from the first one, the color query input to each block is the output of the previous one, representing a refined version of the tensor based on the image features, which are the second input. In our implementation, we used three layers of the color decoder (so, 9 blocks in total).</p>
 <p>Each color input block is a slightly modified version of the basic transformer architecture decoder. First, cross-attention is performed between the color queries (obviously acting as the queries) and the image features (acting as the keys and values). After a skip connection with the input and a layer norm, self attention is performed on the previous output. Then, we have another add and norm, which leads to a simple three-layer MLP, and finally one more add and norm. The output now represents the updated/refined image queries that can be used as input to the next block.</p>
 <h3>Fusion Module</h3>
 <p>Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo. Quisque sit amet est et sapien ullamcorper pharetra. Vestibulum erat wisi, condimentum sed, commodo vitae, ornare sit amet, wisi. Aenean fermentum, elit eget tincidunt condimentum, eros ipsum rutrum orci, sagittis tempus lacus enim ac dui. Donec non enim in turpis pulvinar facilisis. Ut felis. Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat. Aliquam erat volutpat. Nam dui mi, tincidunt quis, accumsan porttitor, facilisis luctus, metus</p>
 <p>Pellentesque malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo.</p>
 <h3>Loss Functions</h3>
 <p>Pellentesque netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo. Quisque sit amet est et sapien ullamcorper pharetra. Vestibulum erat wisi, condimentum sed, commodo vitae, ornare sit amet, wisi. Aenean fermentum, elit eget tincidunt condimentum, eros ipsum rutrum orci, sagittis tempus lacus enim ac dui. Donec non enim in turpis pulvinar facilisis. Ut felis. Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat. Aliquam erat volutpat. Nam dui mi, tincidunt quis, accumsan porttitor, facilisis luctus, metus</p>
 <h4>PixelLoss</h4>
 <p>Pellentesque et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo. Quisque sit amet est et sapien ullamcorper pharetra. Vestibulum erat wisi, condimentum sed, commodo vitae, ornare sit amet, wisi. Aenean fermentum, elit eget tincidunt condimentum, eros ipsum rutrum orci, sagittis tempus lacus enim ac dui. Donec non enim in turpis pulvinar facilisis. Ut felis. Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat. Aliquam erat volutpat. Nam dui mi, tincidunt quis, accumsan porttitor, facilisis luctus, metus</p>
 <p>Pellentesque et netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo.</p>
 <h4>PerceptualLoss</h4>
 <p>Pellentesque morbi tristique senectus et netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo. Quisque sit amet est et sapien ullamcorper pharetra. Vestibulum erat wisi, condimentum sed, commodo vitae, ornare sit amet, wisi. Aenean fermentum, elit eget tincidunt condimentum, eros ipsum rutrum orci, sagittis tempus lacus enim ac dui. Donec non enim in turpis pulvinar facilisis. Ut felis. Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat. Aliquam erat volutpat. Nam dui mi, tincidunt quis, accumsan porttitor, facilisis luctus, metus</p>
 <p>Pellentesque et netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo.</p>
 <h4>AdversarialLoss</h4>
 <p>Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo. Quisque sit amet est et sapien ullamcorper pharetra. Vestibulum erat wisi, condimentum sed, commodo vitae, ornare sit amet, wisi. Aenean fermentum, elit eget tincidunt condimentum, eros ipsum rutrum orci, sagittis tempus lacus enim ac dui. Donec non enim in turpis pulvinar facilisis. Ut felis. Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat. Aliquam erat volutpat. Nam dui mi, tincidunt quis, accumsan porttitor, facilisis luctus, metus</p>
 <p>Pellentesque Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo.</p>
 <h4>ColorfulnessLoss</h4>
 <p>Pellentesque turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo. Quisque sit amet est et sapien ullamcorper pharetra. Vestibulum erat wisi, condimentum sed, commodo vitae, ornare sit amet, wisi. Aenean fermentum, elit eget tincidunt condimentum, eros ipsum rutrum orci, sagittis tempus lacus enim ac dui. Donec non enim in turpis pulvinar facilisis. Ut felis. Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat. Aliquam erat volutpat. Nam dui mi, tincidunt quis, accumsan porttitor, facilisis luctus, metus</p>
 <p>Pellentesque tristique senectus et netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo.</p>
</section>

<section id="findings">
 <h1>4. Experimental Findings</h1>
 <h4>TODO: Add pictures of the model's colorizations.</h4>
 <h4>TODO: Add columns for the original paper's performance.</h4>
 <h5>\tTODO: Claim that our performance is somehow comparable.</h5>
 <p>Our experiments yielded the following results:</p>
<table>
  <tr>
    <th>Checkpoint&nbsp;</th>
    <th>Average&nbsp;FID&nbsp;&#177;&nbsp;0.001</th>
    <th>Average&nbsp;Colorfulness&nbsp;Score&nbsp;&#177;&nbsp;0.001</th>
    <th>Average&nbsp;PSNR&nbsp;&#177;&nbsp;0.001</th>
  </tr>
  <tr>
    <td>1</td>
    <td>8.359</td>
    <td>0.003</td>
    <td>40.078</td>
  </tr>
  <tr>
    <td>2</td>
    <td>28.422</td>
    <td>0.006</td>
    <td>36.257</td>
  </tr>
  <tr>
    <td>3</td>
    <td>11.320</td>
    <td>0.003</td>
    <td>45.376</td>
  </tr>
  <tr>
    <td>4</td>
    <td>10.304</td>
    <td>0.003</td>
    <td>44.706</td>
  </tr>
  <tr>
    <td>5</td>
    <td>9.084</td>
    <td>0.001</td>
    <td>41.716</td>
  </tr>
  <tr>
    <td>6</td>
    <td>5.206</td>
    <td>0.001</td>
    <td>46.669</td>
  </tr>
  <tr>
    <td>7</td>
    <td>7.737</td>
    <td>0.002</td>
    <td>41.932</td>
  </tr>
  <tr>
    <td>8</td>
    <td>8.711</td>
    <td>0.003</td>
    <td>44.707</td>
  </tr>
</table>
</section>

<section id="conclusion">
  <h1>5. Conclusion</h1>
  <p>Conclusion content goes here...</p>
</section>

<h2>Literature Review; Biography; Social Impact; Industry Applications; Follow-on Research; and Peer-Review</h2>

<p>Just as we have done in the role-playing exercise, analyze the paper from all perspectives.
</p>

<p>Optionally, in addition to a reading-based analysis, implement the ideas of the paper in code, and report on your findings.
</p>

<h3>References</h3>

<p><a name="kang2023ddcolor">[1]</a> <a href="https://arxiv.org/abs/2212.11613"
  >Xiaoyang Kang, Tao Yang, Wenqi Ouyang, Peiran Ren, Lingzhi Li, Xuansong Xie.
  <em>DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders.</em></a>
  International Conference on Computer Vision (ICCV) 2023.
</p>

<p><a name="liu2022convnet">[2]</a> <a href="https://arxiv.org/abs/2201.03545"
  >Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.
  <em>A ConvNet for the 2020s.</em></a>
  The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023.
</p>



<h2>Team Members</h2>
                                                   
<p>Hamza Tahboub, Andrew Stelmach</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
